Calcolo della loss per il training, la validation e il test.

TRAINING
Viene calcolata all'interno del ciclo principale di addestramento [riga 268 - supervised_train.py] nel metodo train
- Costruzione del feed dictionary, per permettere il passaggio dei dati al modello [270-271]
- Esecuzione del passo di training, con operazioni di ottimizzazione, calcolo della loss e predizione [riga 275]:

	+ Funzione di loss: calcola la train loss per il batch attuale
	Procedimento [riga 102 - supervised_models.py]: composta da due parti principali. 
	(1) Weight Decay Loss [riga 103]: aggiunge una componente di penalizzazione L2 per i pesi del modello per evitare overfitting. La penalizzazione viene applicata sia ai pesi (vars) degli aggregatori del modello, sia ai pesi (vars) utilizzati per la predizione finale. 
	Il calcolo prevede la moltiplicazione di l2_loss, il quadrato della norma L2 (somma dei quadrati dei valori del tensore), per un fattore di regolarizzazione (FLAGS.weight_decay) che controlla quanto influisce la regolarizzazione nel calcolo della perdita complessiva [righe 106 e 108].
	(2) Classification Loss [riga 110]: viene calcolata la perdita di classificazione utilizzando la cross-entropy, per confrontare i logits con le etichette vere. 
	Se sigmoid_loss è True (classificazione binaria  o multilabel), viene utilizzata la sigmoid_cross_entropy_with_logits. Come parametri vengono usati i logits, che sono i valori grezzi del modello non ancora passati attraverso una funzione di attivazione (come la sigmoide che comprime i logits in un intervallo tra 0 e 1), e le labels, che sono le etichette fornite dal placeholder. Quindi tf.reduce_mean() permette di fare la media della loss su tutti i campioni del minibatch.
	Se sigmoid_loss è False (classificazione multiclass), viene utilizzata la softmax_cross_entropy_with_logits. Il resto è uguale alla soluzione precedente.

	+ Ottimizzazione: minimizza la funzione di loss. 
	Procedimento: Dopo aver calcolato la loss, vengono calcolati i gradienti rispetto ai parametri del modello [riga 95 - supervised_models.py]. Successivamente viene eseguito il gradient clipping [riga 96], per far rimanere il gradiente all'interno di un certo limite per evitare aggiornamenti drastici dei pesi, che potrebbero rovinare il training. Si fa anche un controllo se i gradienti sono None, cioè non esistono per alcune variabili. Il primo gradiente della lista viene quindi salvato su self.grad [riga 98]. Infine con self.opt_op [riga 99], l'ottimizzatore usa i gradienti per aggiornare i parametri del modello e ridurre la funzione di loss.
	
	+ Predizione: fornisce le predizioni del modello per il batch attuale [riga 122 - supervised_models.py]. 
	Procedimento: Le predizioni del modello vengono generate con una funzione di attivazione (sigmoid o softmax) applicata ai valori grezzi prodotti dal modello (cioè i self.node_preds = logits).

	+ Con l'avvio della sessione [riga 275 - supervised_train.py] si ha quindi un forward pass, in cui le caratteristiche dei nodi vengono propagate attraverso i vari strati del modello utilizzando gli aggregatori definiti [riga 278 - models.py].

Il ciclo termina all'esaurimento dei minibatch.


----------------------------------------------------------------------------------------------------------------------
VALIDATION
 













Graphsage, supervised_train.

tf.app.run(): avvia l'esecuzione del programma tensorflow[v1.8] (costruisce un grafo computazionale) e procede con la funzione main(). Inoltre esegue il parsing degli argomenti.

main: carica i dati di training (gestiti con FLAGS) e avvia l'addestramento (train()). 

train: 
	vengono caricati i dati (G, features, id_map, class_map).
	verifica se i valori di class_map sono liste e calcola il numero di classi.
	se i nodi hanno delle features, viene aggiunta una riga di zeri alla matrice delle features (per gestire i nodi che non hanno features assegnate)
	context_pairs vengono incluse nell'argomento se random_context è su true
	placeholders sono variabili speciali utilizzate in tensorflow per rappresentare degli input.
	minibatch, un iteratore che seleziona un sottoinsieme dei nodi e delle loro connessioni per ogni passo di addestramento. Ha diversi parametri.
	adj_info_ph, placeholder per la matrice  di adiacenza, rappresenta le connessioni tra i nodi.
	adj_info variabile che rappresenta la matrice di adiacenza (no addestrabile)

	Viene scelto il modello in base al parametro FLAGS.model
	1. graphsage_mean:
		sampler: campionatore che seleziona i vicini dei nodi in modo casuale, in base a adj_info
		Layer Infos: vengono creati oggetti che rappresentano informazioni per ciascun layer del modello
		modello: viene creato il modello usando la classe SupervisedGraphsage (importato da supervised_models), con tutti i dati in questione.
	2. gcn (Graph Convolutional Network):
		differenze col precedente: l'uso dell'aggregatore specifico "gcn" come parametro del modello e i layer usano dimensioni doppie.
	3. Altri modelli (graphsage_seq, graphsage_maxpool, graphsage_meanpool):
		processo uguale al primo, quello che cambio èil tipo di aggregatore utilizzato per il modello
	Configurazione di TF: con tf.ConfigProto e altre cose.

	Inizializzazione della sessione di TensorFlow: viene creata una sessione TF sess, merge dei tensori di riepilogo definiti in precedenza e viene creato un writer per il log.
	Inizializzazione delle variabili: sia quelle globali e le variabili di adiacenza del grafo.
	



	Ciclo di addestramento e validazione: avviene per un numero definito di epoche (flags)
	1. inizializzazione delle variabili per l'addestramento (step totali, tempo medio e costo di validazione per epoca)
	2. matrici di adiacenza impostate per allenamento e validazione
	3. il dataset (minibatch) viene mescolato così vengono forniti in ordine casuale.
	


	4. Nel ciclo principale di addestramento (quindi per ogni minibatch):
		viene estratto il prossimo minibatch e crea il dizionario feed_dict per passare i dati al modello
		aggiunge la possibilità di dropout al feed_dict.
		sess.run: esegue il grafo di addestramento focalizzandosi sulla classe model (qua viene calcolata la perdita (LOSS) del minibatch corrente e salvata su train_cost), 
			model.opt_op: esegue un passo di ottimizzazione che minimizza la funzione di perdita. Come funziona? Dopo aver calcolato la loss, vengono calcolati i gradienti rispetto ai parametri del modello (compute_gradients() in supervised_models.py). Successivamente viene eseguito il gradient clipping (clipped_grads_and_vars), per far rimanere il gradient all'interno di un certo limite per evitare aggiornamenti drastici dei pesi, che potrebbero rovinare il training. Successivamente viene fatto un controllo se i gradienti sono None, cioè non esistono per alcune variabili. Il primo gradiente della lista viene quindi salvato su self.grad. Infine con self.opt_op, l'ottimizzatore usa i gradienti per aggiornare i parametri del modello e ridurre la funzione di perdita (eseguito da apply_gradients() dell'ottimizzatore).
			model.loss: calcola la train loss per il batch attuale (spiegata giù)
			model.preds: fornisce le predizioni del modello per il batch attuale. Come funziona? Le predizioni del modello vengono generate con la funzione self.predict(), che restituisce il risultato di una funzione di attivazione (nel nostro caso la sigmoid) applicata ai valori grezzi prodotti dal modello (cioè i self.node_preds).


	
	5. validazione: ogni x minibatch, viene eseguita una validazione (la x è impostata tramite la flag "validate_iter") 
		matrice di adiacenza impostata
		incremental_evaluate() per la valutazione incrementale su tutto il dataset di validazione, usata quando il batch di validazione non ha un limite fisso
		con evaluate(), valutazione diretta con batch di validazione di dimensione fissa, calcolo la loss, f1_micro e macro della validazione
		ripristina la matrice di adiacenza ai dati di addestramento
		Accumula la perdita di validazione nel costo totale dell'epoca corrente. 
	6. ogni x interazioni (impostata da una flag) vengono calcolate f1_micro e f1_macro usando la funzione calc_f1.
	fine addestramento

	vengono salvate le metriche e tempi di validazione e di test







Considerando il modello di tipo graphsage_mean, vediamo nel dettaglio il calcolo delle metriche.

Metriche di addestramento (train)
LOSS: calcolata quando procede con l'operazione sess.run e che include model.loss. Viene calcolata in due fasi principali:
	1. Perdita dovuta alla regolarizzazione L2: aggiunge una componente di regolarizzazione al modello per prevenire l'overfitting, penalizzando i pesi di grande magnitudine. La regolarizzazione L2 viene applicata alle variabili (i pesi) degli aggregatori del modello e alle variabili (i pesi) utilizzati per la predizione finale. la l2_loss è il quadrato della norma L2 (somma dei quadrati dei valori del tensore) e viene moltiplicato per un fattore di regolarizzazione (un flag) che controlla quanto influisce la regolarizzazione nel calcolo della perdita complessiva.
	2. Perdita di classificazione: 
		a. per classificazione binaria  o multilabel (se sigmoid_loss è True), viene calcolata la perdita con tf.nn.sigmoid_cross_entropy_with_logits(logits, labels) (cross-entropy per ogni campione): logit  sono le uscite grezze del modello non ancora passate attraverso una funzione di attivazione (come la sigmoide che comprime i logits in un intervallo tra 0 e 1), le labels sono le etichette fornite dal placeholder. Infine con tf.reduce_mean() viene fatta la media  della perdita su tutti i campioni del minibatch.
		b. per classificazione multiclass (non mi interessa per il momento)

	Infine viene aggiunto il valore corrente di self.loss ai log di TensorBoard, per monitorare l'andamento della perdita durante l'allenamento. Viene creato un riassunto scalare del valore self.loss, visualizzabile con TensorBoard.

F1_MICRO e F1_MACRO: 
	Viene usata direttamente la funzione calc_f1:
		se non è sigmoid (quindi è softmax per la classificazione multiclasse) si convertono le predizioni e le etichette vere negli indici delle classi predette usando np.argmax.
		se è sigmoid (classificazione multi-label), si applica una soglia di 0.5 per decidere se un'etichetta è attiva (1) o no (0).
		usa metrics.f1_score() dalla libreria sklearn per calcolare le metriche f1 micro e macro.


Metriche di validazione: calcolate all'interno delle funzioni incremental_evaluate o evaluate.
	Funzione evaluate:
		imposta timer
		crea il feed_dict che contiene i dati di input necessari per la validazione e ottiene anche le labels corrispondenti al minibatch
		con sess_run() calcola le predizioni e la perdita (LOSS), considerando i dati di validazione forniti dal dizionario
		chiama calc_f1() per calcolare le metriche F1 MICRO e F1 MACRO, utilizzando le etichette vere e le predizioni calcolate
		restituisce la perdita, f1 micro e macro e il tempo
	Funzione incremental_evaluate:
		imposta timer e inizializza le liste
		ciclo sui batch: 
			ottiene il dizionario, le etichette e un flag che indica se tutti i batch sono stati processati
			con sess_run() calcola le predizioni e la perdita (LOSS) su quel batch.
			continua fino a quando finished diventa true
		tutte le predizioni e le etichette vengono concatenate per avere un unico array
		viene usato calc_f1()
		resituisce la media deli LOSS, f1 micro e macro e il tempo impegato


sigmoid: È usata nei problemi di multi-label classification, dove ogni esempio può appartenere a più di una classe. Ad esempio, un'immagine può essere etichettata sia come "gatto" che come "cane" contemporaneamente.

softmax: È usata nei problemi di multi-class classification, dove ogni esempio può appartenere solo a una singola classe. Ad esempio, un'immagine può essere classificata come "gatto", "cane", o "uccello", ma solo una di queste.


	
		





















	